{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d9c5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# LANGCHAIN_TRACING_V2=true\n",
    "# LANGCHAIN_API_KEY=\n",
    "# OPENAI_API_KEY=\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c14ed2",
   "metadata": {},
   "source": [
    "## simple test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90c625f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import (\n",
    "    RunnablePassthrough, \n",
    "    RunnableLambda, \n",
    "    RunnableParallel\n",
    ")\n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = 'lcel_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23246656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why did the ice cream truck break down? It just couldn't handle the rocky road!\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "\n",
    "# 创建模型实例\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# lcel\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "789be502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Tell me a short joke about ice cream', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({'topic': 'ice cream'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebb4b527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why did the ice cream truck break down?\\nIt couldn't handle the rocky road!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 15, 'total_tokens': 32, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'id': 'chatcmpl-C7feid19hfJF5LLL1SoIpDqMft8j2', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--b72ca7c5-2002-4adc-a1e5-5c324dc69fdf-0', usage_metadata={'input_tokens': 15, 'output_tokens': 17, 'total_tokens': 32, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(prompt.invoke({'topic': 'ice cream'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8794043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream truck break down?\\n\\nBecause it had too many sundae drivers!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(llm.invoke(prompt.invoke({'topic': 'ice cream'})))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152480d1",
   "metadata": {},
   "source": [
    "RunnablePassthrough()\n",
    "\n",
    "拿到什么，输出就是什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6206138a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnablePassthrough() | RunnablePassthrough () | RunnablePassthrough ()\n",
    "chain.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71eb13d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HELLO'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnablePassthrough() | RunnableLambda(lambda x: x.upper())\n",
    "chain.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11247483",
   "metadata": {},
   "source": [
    "## json test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d35716d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_PROJECT\"] = 'json_test2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13f27a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.prompts.chat import SystemMessagePromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "545d027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78102a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_parser = JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fafa7fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建提示模板\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", '''I want you to extract the person name, age and a description from the following text.\n",
    "    Here is the JSON object, output:\n",
    "    {{\n",
    "        \"name\": string,\n",
    "        \"age\": int,\n",
    "        \"description\": string\n",
    "    }}'''),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# 创建 LCEL 链\n",
    "chain = (\n",
    "    {\"input\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm \n",
    "    | json_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df222a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='I want you to extract the person name, age and a description from the following text.\\n    Here is the JSON object, output:\\n    {{\\n        \"name\": string,\\n        \"age\": int,\\n        \"description\": string\\n    }}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74751b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='I want you to extract the person name, age and a description from the following text.\\n    Here is the JSON object, output:\\n    {{\\n        \"name\": string,\\n        \"age\": int,\\n        \"description\": string\\n    }}') additional_kwargs={}\n",
      "prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}') additional_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "print(prompt[0])\n",
    "print(prompt[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40a2f53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='I want you to extract the person name, age and a description from the following text.\\n    Here is the JSON object, output:\\n    {{\\n        \"name\": string,\\n        \"age\": int,\\n        \"description\": string\\n    }}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatPromptTemplate.from_messages(\n",
    "    [SystemMessagePromptTemplate.from_template('''I want you to extract the person name, age and a description from the following text.\n",
    "    Here is the JSON object, output:\n",
    "    {{\n",
    "        \"name\": string,\n",
    "        \"age\": int,\n",
    "        \"description\": string\n",
    "    }}'''), \n",
    "     HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91370788",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke(\"John is 20 years old. He is a student at the University of California, Berkeley. He is a very smart student.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75a48140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'John',\n",
       " 'age': 20,\n",
       " 'description': 'He is a student at the University of California, Berkeley. He is a very smart student.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dac6a5f",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6512f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = 'rag_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d39a6313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42a2d618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x144960ac0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x1449609a0>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base='https://api.zhizengzeng.com/v1', openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    api_key=\"sk-zk269f115f79bce049181cf14a96b870acf68a9952257e6d\",  # 替换为你的实际 API Key\n",
    "    openai_api_base=\"https://api.zhizengzeng.com/v1\",  # 自定义 base_url\n",
    "    # 可选参数\n",
    "    # chunk_size=1,  # 根据代理要求调整\n",
    "    # check_embedding_ctx_length=False,  # 如果报错超长，可关闭检查\n",
    ")\n",
    "\n",
    "embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c20e32b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_texts(\n",
    "    [\"Cats love thuna\"], embedding=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbefb431",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2dadbea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='d8a0029c-c5db-4c01-9f1b-d15e8b187583', metadata={}, page_content='Cats love thuna')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What do cats like to eat?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7627e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0faa4287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tuna'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What do cats like to eat?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faff359",
   "metadata": {},
   "source": [
    "## tool use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1c9b48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_PROJECT\"] = 'tools_test2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2eeb390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90db7db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(num1: float, num2: float) -> float:\n",
    "    \"Add two numbers.\"\n",
    "    return num1 + num2\n",
    "    \n",
    "@tool\n",
    "def subtract(num1: float, num2: float) -> float:\n",
    "    \"\"\"\n",
    "    Subtract two numbers.\n",
    "    \"\"\"\n",
    "    return num1 - num2\n",
    "    \n",
    "@tool\n",
    "def multiply(num1: float, num2: float) -> float:\n",
    "    \"\"\"Multiply two float .\"\"\"\n",
    "    return num1 * num2\n",
    "\n",
    "@tool\n",
    "def divide(numerator: float, denominator: float) -> float:\n",
    "    \"\"\"\n",
    "    Divides the numerator by the denominator.\n",
    "    \"\"\"\n",
    "\n",
    "    result = numerator / denominator\n",
    "    return result\n",
    "\n",
    "@tool\n",
    "def power(base: float, exponent: float) -> float:\n",
    "    \"Take the base to the exponent power, base^exponent.\"\n",
    "    return base**exponent\n",
    "\n",
    "\n",
    "@tool\n",
    "def exp(x):\n",
    "    \"\"\"\n",
    "    Calculate the natural exponential $e^x$\n",
    "    \"\"\"\n",
    "    return np.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0b85fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [add, subtract, multiply, divide, power, exp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d4926f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aed6794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, streaming=True)\n",
    "\n",
    "system_template = \"\"\"\n",
    "You are a helpful math assistant that uses calculation functions to solve complex math problems step by step.\n",
    "\"\"\"\n",
    "\n",
    "human_template = \"{input}\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_template),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n",
    "        HumanMessagePromptTemplate.from_template(input_variables=[\"input\"], template=human_template),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95e7c0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "034a261d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `exp` with `{'x': -2.5}`\n",
      "responded: The derivative of the sigmoid function, often denoted as \\(\\sigma(x)\\), is given by the formula:\n",
      "\n",
      "\\[\n",
      "\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))\n",
      "\\]\n",
      "\n",
      "where \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\).\n",
      "\n",
      "To find the derivative of the sigmoid function at \\(x = 2.5\\), we need to:\n",
      "\n",
      "1. Calculate \\(\\sigma(2.5)\\).\n",
      "2. Use the derivative formula \\(\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))\\).\n",
      "\n",
      "Let's start by calculating \\(\\sigma(2.5)\\):\n",
      "\n",
      "\\[\n",
      "\\sigma(2.5) = \\frac{1}{1 + e^{-2.5}}\n",
      "\\]\n",
      "\n",
      "Now, let's compute \\(e^{-2.5}\\) and then \\(\\sigma(2.5)\\).\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m0.0820849986238988\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `divide` with `{'numerator': 1, 'denominator': 1.0820849986238987}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m0.9241418199787566\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `subtract` with `{'num1': 1, 'num2': 0.9241418199787566}`\n",
      "responded: The value of \\(\\sigma(2.5)\\) is approximately \\(0.9241\\).\n",
      "\n",
      "Now, let's calculate the derivative \\(\\sigma'(2.5)\\) using the formula:\n",
      "\n",
      "\\[\n",
      "\\sigma'(2.5) = \\sigma(2.5) \\cdot (1 - \\sigma(2.5))\n",
      "\\]\n",
      "\n",
      "Substituting the value we found:\n",
      "\n",
      "\\[\n",
      "\\sigma'(2.5) = 0.9241 \\cdot (1 - 0.9241)\n",
      "\\]\n",
      "\n",
      "Let's compute this.\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m0.07585818002124345\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `multiply` with `{'num1': 0.9241418199787566, 'num2': 0.07585818002124335}`\n",
      "responded: The value of \\(\\sigma(2.5)\\) is approximately \\(0.9241\\).\n",
      "\n",
      "Now, let's calculate the derivative \\(\\sigma'(2.5)\\) using the formula:\n",
      "\n",
      "\\[\n",
      "\\sigma'(2.5) = \\sigma(2.5) \\cdot (1 - \\sigma(2.5))\n",
      "\\]\n",
      "\n",
      "Substituting the value we found:\n",
      "\n",
      "\\[\n",
      "\\sigma'(2.5) = 0.9241 \\cdot (1 - 0.9241)\n",
      "\\]\n",
      "\n",
      "Let's compute this.\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m0.07010371654510798\u001b[0m\u001b[32;1m\u001b[1;3mThe derivative of the sigmoid function at \\(x = 2.5\\), \\(\\sigma'(2.5)\\), is approximately \\(0.0701\\).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the result of directive of sigmoid(2.5)?',\n",
       " 'output': \"The derivative of the sigmoid function at \\\\(x = 2.5\\\\), \\\\(\\\\sigma'(2.5)\\\\), is approximately \\\\(0.0701\\\\).\"}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What is the result of directive of sigmoid(2.5)?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5441c8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2a9e31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0701])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sigmoid(torch.tensor([2.5])) * (1-F.sigmoid(torch.tensor([2.5])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
